{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression in TensorFlow\n",
    "\n",
    "## 1. Linear Regression\n",
    "\n",
    "### Problem Statement\n",
    "Problem: We often hear insurance companies using factors such as number of fire and theft in a neighborhood to calculate how dangerous the neighborhood is. Is it redundant? Is there a relationship between the number of fire and theft in a neighborhood, and if there is, can we find it?\n",
    "\n",
    "In other words, can we find a function f so that if X is the number of fires and Y is the number of thefts, then: Y = f(X)?\n",
    "\n",
    "Dataset Description:\n",
    "\n",
    "Name: Fire and Theft in Chicago \n",
    "\n",
    "X = fires per 1000 housing units\n",
    "\n",
    "Y = thefts per 1000 population within the same Zip code in the Chicago metro area \n",
    "\n",
    "Total number of Zip code areas: 42\n",
    "\n",
    "### Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "# data file\n",
    "DATA_FILE = 'fire_theft.xls'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Step 1: read in data from the .xls file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** No CODEPAGE record, no encoding_override: will use 'ascii'\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(DATA_FILE, encoding='utf-8')\n",
    "n_samples = df.shape[0]\n",
    "data = df.values\n",
    "print(n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Step2: create placeholders for input X(number of fire) and label Y (number of theft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, name='X')\n",
    "Y = tf.placeholder(tf.float32, name='Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step3: create weight and bias, initialized to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = tf.Variable(0.0, name='weight')\n",
    "b = tf.Variable(0.0, name='bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step4: construct model to predict Y (number of theft) from the number of fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_predict = X*w + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step5: use the square error as the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.square(Y - Y_predict, name='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: using gradient descent with learning rate of 0.01 to minimize loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Step7: initialize the necessary variables, in this case, w and b\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # step 8: train the model\n",
    "    for i in range(100): # run 100 epochs\n",
    "        for x, y in data:\n",
    "            # Session runs train_op to minimize loss\n",
    "            sess.run(optimizer, feed_dict={X:x, Y:y})\n",
    "    # Step 9: output the values of w and b\n",
    "    w_value, b_value = sess.run([w, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x119a1ae80>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119923cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Step 10 plot the results\n",
    "plt.hold\n",
    "plt.scatter(data[:,0],data[:,1], color='b')\n",
    "z = lambda x: w_value*x+b_value\n",
    "plt.plot(range(40), z(range(40)), color='r')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard\n",
    "\n",
    "![tensoflow_graph.png](images/tensoflow_graph.png)\n",
    "\n",
    "TensorFlow will execute the part of the graph that those ops depend on. In this case, we see that `train_op` has the purpose of minimize ops, and loss depends on variables $w$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers\n",
    "\n",
    "GradientDescentOptimizer means that our update rule is gradient descent. TensorFlow does auto differentiation for us, then update the values of w and b to minimize the loss. Autodiff is amazing!\n",
    "\n",
    "By default, the optimizer trains all the trainable variables whose objective function depend on. If there are variables that you do not want to train, you can set the keyword trainable to False when you declare a variable. One example of a variable you don’t want to train is the variable global_step, a common variable you will see in many TensorFlow model to keep track of how many times you’ve run your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.01\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, trainable=False, dtype=tf.int32) \n",
    "learning_rate = 0.01 * 0.99 ** tf.cast(global_step, tf.float32)\n",
    "increment_step = global_step.assign_add(1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate) # learning rate can be a tensor\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(global_step))\n",
    "    print(sess.run(learning_rate))\n",
    "    print(sess.run(increment_step))\n",
    "    print(sess.run(increment_step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of optimizers\n",
    "\n",
    "* tf.train.GradientDescentOptimizer\n",
    "* tf.train.AdadeltaOptimizer \n",
    "* tf.train.AdagradOptimizer \n",
    "* tf.train.AdagradDAOptimizer \n",
    "* tf.train.MomentumOptimizer\n",
    "* tf.train.AdamOptimizer \n",
    "* tf.train.FtrlOptimizer \n",
    "* tf.train.ProximalGradientDescentOptimizer \n",
    "* tf.train.ProximalAdagradOptimizer\n",
    "* tf.train.RMSPropOptimizer\n",
    "\n",
    "\n",
    "`RMSprop` is an extension of `Adagrad` that deals with its radically diminishing learning rates. It is identical to `Adadelta`, except that `Adadelta` uses the RMS of parameter updates in the numerator update rule. `Adam`, finally, adds bias-correction and momentum to `RMSprop`. Insofar, `RMSprop`, `Adadelta`, and `Adam` are very similar algorithms that do well in similar circumstances. Kingma et al. [15] show that its bias-correction helps `Adam` slightly outperform `RMSprop` towards the end of optimization as gradients become sparser. Insofar, `Adam` might be the best overall choice.”\n",
    "\n",
    "**TL;DR**: Use AdamOptimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic Regression in TensorFlow: MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "The `MNIST` database (Mixed National Institute of Standards and Technology database) is probably one of the most popular databases used for training various image processing systems. It is a database of handwritten digits. The images look like this:\n",
    "\n",
    "![mnist](images/mnist.png)\n",
    "\n",
    "Each image is 28 x 28 pixels, flatten to be a 1-d tensor of size 784. Each comes with a label. For example, images on the first row is labelled as 0, the second as 1, and so on. The dataset is hosted on Yann Lecun’s website (http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "### One-hot encoding\n",
    "\n",
    "One-hot encoding In digital circuits, one-hot refers to a group of bits among which the legal combinations of values are only those with a single high (1) bit and all the others low (0).\n",
    "\n",
    "In this case, one-hot encoding means that if the output of the image is the digit 7, then the output will be encoded as a vector of 10 elements with all elements being 0, except for the element at index 7 which is 1.\n",
    "\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 2.3031328]\n",
      "[None, 2.2859359]\n",
      "[None, 2.2878366]\n",
      "[None, 2.2709088]\n",
      "[None, 2.2592759]\n",
      "[None, 2.2387748]\n",
      "[None, 2.2431116]\n",
      "[None, 2.2036476]\n",
      "[None, 2.2120895]\n",
      "[None, 2.204885]\n",
      "[None, 2.1988001]\n",
      "[None, 2.187006]\n",
      "[None, 2.1816993]\n",
      "[None, 2.1794415]\n",
      "[None, 2.143878]\n",
      "[None, 2.1666057]\n",
      "[None, 2.1246254]\n",
      "[None, 2.1375103]\n",
      "[None, 2.1084881]\n",
      "[None, 2.0930595]\n",
      "[None, 2.1304812]\n",
      "[None, 2.0817747]\n",
      "[None, 2.0906787]\n",
      "[None, 2.1002779]\n",
      "[None, 2.0839019]\n",
      "Accuracy 0.744\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Step 1: Read in data \n",
    "# using TF Learn's built in function to load MNIST data to the folder data/mnist \n",
    "# MNIST = input_data.read_data_sets(\"mnist\", one_hot=True)\n",
    "\n",
    "# Step 2: Define parameters for the model \n",
    "learning_rate = 0.01 \n",
    "batch_size = 128 \n",
    "n_epochs = 25\n",
    "\n",
    "# Step 3: create placeholders for features and labels \n",
    "# each image in the MNIST data is of shape 28*28 = 784 \n",
    "# therefore, each image is represented with a 1x784 tensor \n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# each label is one hot vector.\n",
    "\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784]) \n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10])\n",
    "\n",
    "# Step 4: create weights and bias \n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01 \n",
    "# b is initialized to 0 \n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w) \n",
    "# shape of b depends on Y \n",
    "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name=\"weights\") \n",
    "b = tf.Variable(tf.zeros([1, 10]), name=\"bias\")\n",
    "\n",
    "# Step 5: predict Y from X and w, b \n",
    "# the model that returns probability distribution of possible label of the image \n",
    "# through the softmax layer \n",
    "# a batch_size x 10 tensor that represents the possibility of the digits \n",
    "logits = tf.matmul(X, w) + b\n",
    "\n",
    "# Step 6: define loss function \n",
    "# use softmax cross entropy with logits as the loss function \n",
    "# compute mean cross entropy, softmax is applied internally \n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits) \n",
    "loss = tf.reduce_mean(entropy) # computes the mean over examples in the batch\n",
    "\n",
    "# Step 7: define training op \n",
    "# using gradient descent with learning rate of 0.01 to minimize cost \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) \n",
    "    n_batches = int(MNIST.train.num_examples/batch_size) \n",
    "    for i in range(n_epochs): # train the model n_epochs times for _ in range(n_batches):\n",
    "        X_batch, Y_batch = MNIST.train.next_batch(batch_size) \n",
    "        print(sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch}))\n",
    "    # average loss should be around 0.35 after 25 epochs\n",
    "    \n",
    "    # test the model \n",
    "    n_batches = int(MNIST.test.num_examples/batch_size) \n",
    "    total_correct_preds = 0 \n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = MNIST.test.next_batch(batch_size) \n",
    "        _, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "        preds = tf.nn.softmax(logits_batch) \n",
    "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32)) # similar to numpy.count_nonzero(boolarray) :(\n",
    "        total_correct_preds += sess.run(accuracy)\n",
    "    \n",
    "    print(\"Accuracy {0}\".format(total_correct_preds/MNIST.test.num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "#mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open interactive session\n",
    "import tensorflow as tf\n",
    "# If you are not using an InteractiveSession, \n",
    "# then you should build the entire computation graph before starting a session and launching the graph.\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up variables and initialize\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute cross entropy\n",
    "\n",
    "y = tf.matmul(x,W) + b\n",
    "# tf.nn.softmax_cross_entropy_with_logits internally applies the softmax on the model's unnormalized model prediction and sums across all classes\n",
    "# tf.reduce_mean takes the average over these sums.\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use steepest gradient descent, with a step length of 0.5, to descend the cross entropy\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "for _ in range(1000):\n",
    "    # load 100 training examples in each training iteration\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9171\n"
     ]
    }
   ],
   "source": [
    "# check if our prediction matches the truth.\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "\n",
    "# cast to floating point numbers and then take the mean\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# accueacy\n",
    "print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.16\n",
      "step 100, training accuracy 0.88\n",
      "step 200, training accuracy 0.94\n",
      "step 300, training accuracy 0.96\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-5ed65716da9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m   print('test accuracy %g' % accuracy.eval(feed_dict={\n\u001b[0;32m---> 69\u001b[0;31m       x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n\u001b[0m",
      "\u001b[0;32m/Users/larry/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     \"\"\"\n\u001b[0;32m--> 541\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/larry/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4083\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4084\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 4085\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/larry/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/larry/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/larry/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/larry/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/larry/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Weight Initialization\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    # Computes a 2-D convolution given 4-D input and filter tensors.\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    # Performs the max pooling on the input.\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# First Convolutional Layer\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# Second Convolutional Layer\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "\n",
    "# Densely Connected Layer\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "\n",
    "# Dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# Readout Layer\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "# Train and Evaluate the Model\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  for i in range(400):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i % 100 == 0:\n",
    "      train_accuracy = accuracy.eval(feed_dict={\n",
    "          x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "      print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "  print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "      x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
