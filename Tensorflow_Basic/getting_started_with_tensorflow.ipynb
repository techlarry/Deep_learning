{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Core tutorial\n",
    "### importing TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Computational Graph\n",
    "\n",
    "A `computational graph` is a series of TensorFlow operations arranged into a graph of nodes. Let's build a simple computational graph. Each node takes zero or more tensors as inputs and produces a tensor as an output. One type of node is a constant. Like all TensorFlow constant, it takes no inputs, and it outputs a value it stores internally. We can create two floating point Tensor `node1` and `node2` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=float32) Tensor(\"Const_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "node1 = tf.constant(3.0, dtype=tf.float32)\n",
    "node2 = tf.constant(4.0) # also tf.float32 implicitly\n",
    "print(node1, node2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce output values for `node1` and `nodes2`, evaluation is needed. To actually evaluate the nodes, we must run the computational graph within a `session`. A session encapsulates the control and state of the Tensorflow runtime.\n",
    "\n",
    "The following code creates a `Session` object and then invokes its `run` method to run enough of the computational graph to evaluate `node1` and `node2`. By running the computational graph in a session as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run([node1, node2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build more complicated computations by combining `Tensor` nodes with operations(Operations are also nodes). For example, we can add our two constant nodes and produce a new graph as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node3: Tensor(\"Add:0\", shape=(), dtype=float32)\n",
      "sess.run(node3): 7.0\n"
     ]
    }
   ],
   "source": [
    "node3 = tf.add(node1, node2)\n",
    "print('node3:', node3)\n",
    "print('sess.run(node3):', sess.run(node3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow provides a utility called `TensorBoard` that can display a picture of the computational graph. Here is a screenshot showing how `TensorBoard` visualizes the graph:\n",
    "![](images/getting_started_add.png)\n",
    "\n",
    "As it stands, this graph is not especially interesting because it always produces a constant result. A graph can be parameterized to accept external inputs, known as `placeholders`. A `placeholder` is a promise to provide a value later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b # + provides a shortcut for tf.add(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding three lines are a bit like a function or a lambda in which we define two input parameters (a and b) and then an operation on them. We can evaluate this graph with multiple inputs by using the feed_dict argument to the `run method` to feed concrete values to the placeholder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[ 3.  7.]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(adder_node, {a:3, b:4.5}))\n",
    "print(sess.run(adder_node, {a:[1,3], b:[2,4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `TensorBoard`, the graph looks like this:\n",
    "\n",
    "![](images/getting_started_adder.png)\n",
    "\n",
    "We can make the computational graph more complex by adding another operation. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.5\n"
     ]
    }
   ],
   "source": [
    "add_and_triple = adder_node * 3.\n",
    "print(sess.run(add_and_triple,{a:3, b:4.5}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding computational graph would look as follows in `TensorBoard`:\n",
    "![](images/getting_started_triple.png)\n",
    "\n",
    "In machine learning we will typically want a model that can take arbitrary inputs, such as the one above. To make the model trainable, we need to be able to modify the graph to get new outputs with the same input. `variables` allow us to add trainable parameters to a graph. They are constructed with a type and initial value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable([.3], dtype=tf.float32)\n",
    "b = tf.Variable([-.3], dtype=tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W*x +b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants are initialized when you call `tf.constant`, and their value can never change. By contrast, variables are not initialized when you call `tf.Variable`. To initialize all the variables in a TensorFlow program, you must explicitly call a special operation as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to realize `init` is a handle to the TensorFlow sub-graph that initializes all the global variables. Until we call `sess.run`, the variables are uninitialized.\n",
    "\n",
    "Since `x` is a placeholder, we can evaluate `linear_model` for several values of `x` simultaneously as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.30000001  0.60000002  0.90000004]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(linear_model,{x:[1,2,3,4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've create a model, but we don't know how good it is yet. To evaluate the model on training data, we need a `y` placeholder to provide the desired values, and we need to write a loss function.\n",
    "\n",
    "A `loss function` measures how far apart the current model is from the provided data. We'll use a standard loss model for linear regression, which sums the squares of the deltas between the current model and the provided data. `linear_model -y` creates a vector where each element is the corresponding example's error delta. We call `tf.square` to square that error. Then, we sum all the squared errors to create a single scalar that abstacts the error of all examples using `tf.reduce_sum`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.66\n"
     ]
    }
   ],
   "source": [
    "y = tf.placeholder(tf.float32)\n",
    "squared_deltas = tf.square(linear_model - y)\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could improve this manually by reassigning the values of `W` and `b` to the perfect values of -1 and 1. A variable is initialized to the value provided to `tf.Variable` but can be changed using operations like `tf.assign`. FOr example, `W=-1` and `b=1` are the optimal parameters for our model. We can change `W` and `b` accordingly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "fixW = tf.assign(W, [-1.])\n",
    "fixb = tf.assign(b, [1.])\n",
    "sess.run([fixW, fixb])\n",
    "print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We guessed the \"perfect\" values of `W` and `b`, but the whole point of machine learning is to find the correct model parameters automatically. We will show how to accomplish this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.train API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A complete discussion of machine learning is out of the scope of this tutorial. However, TensorFlow provides `optimizers` that slowly change each variable in order to minimize the loss function. The simplest optimizer is `gradient descent`. It modifies each variable according to the magnitude of the derivative of loss with respect to that variable. In general, computing symbolic derivatives manually is tedious and error-prone. Consequently, `TensorFlow` can automatically produce derivatives given only a description of the model using the function `tf.gradients`. For simplicity, `optimizers` typically do this for you. For example,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.9999969], dtype=float32), array([ 0.99999082], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "sess.run(init) # reset values to incorrect defaults\n",
    "for i in range(1000):\n",
    "    sess.run(train, {x:[1,2,3,4], y:[0,-1,-2,-3]})\n",
    "print(sess.run([W,b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have done actual machine learning! Although doing this simple linear regression doesn't require much TensorFlow core code, more complicated models and methods to feed data into your model necessitate more code. Thus TensorFlow provides higher level abstractions for common patterns, structures, and functionality. We will learn how to use some of these abstractions in the next section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [-0.9999969] b: [ 0.99999082] loss: 5.69997e-11\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Model parameters\n",
    "W = tf.Variable([.3], dtype=tf.float32)\n",
    "b = tf.Variable([-.3], dtype=tf.float32)\n",
    "# Model input and output\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W * x + b\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# loss\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y)) # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# training data\n",
    "x_train = [1, 2, 3, 4]\n",
    "y_train = [0, -1, -2, -3]\n",
    "# training loop\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init) # reset values to wrong\n",
    "for i in range(1000):\n",
    "  sess.run(train, {x: x_train, y: y_train})\n",
    "\n",
    "# evaluate training accuracy\n",
    "curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x: x_train, y: y_train})\n",
    "print(\"W: %s b: %s loss: %s\"%(curr_W, curr_b, curr_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This more complicated program can still be visualized in TensorBoard:\n",
    "\n",
    "![](images/getting_started_final.png)\n",
    "\n",
    "## tf.estimator\n",
    "\n",
    "`tf.estimator` is a high-level TensorFlow library that simplifies the mechanics of machine learning, including the following:\n",
    "\n",
    "* running training loops\n",
    "* running evaluation loops\n",
    "* managing data sets\n",
    "* `tf.estimator` defines many common models.\n",
    "\n",
    "### Basic usage\n",
    "\n",
    "Notice how much simpler the linear regression program becomes with `tf.estimator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/66/y1hc77j572v71r0gm2r39rfr0000gn/T/tmpaqwcnezd\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/66/y1hc77j572v71r0gm2r39rfr0000gn/T/tmpaqwcnezd', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/66/y1hc77j572v71r0gm2r39rfr0000gn/T/tmpaqwcnezd/model.ckpt.\n",
      "INFO:tensorflow:loss = 14.0, step = 1\n",
      "INFO:tensorflow:global_step/sec: 835.386\n",
      "INFO:tensorflow:loss = 0.0836052, step = 101 (0.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 699.129\n",
      "INFO:tensorflow:loss = 0.0214934, step = 201 (0.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 691.171\n",
      "INFO:tensorflow:loss = 0.0107881, step = 301 (0.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 681.389\n",
      "INFO:tensorflow:loss = 0.00441465, step = 401 (0.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 530.203\n",
      "INFO:tensorflow:loss = 0.000371181, step = 501 (0.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 454.207\n",
      "INFO:tensorflow:loss = 7.16481e-05, step = 601 (0.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 687.296\n",
      "INFO:tensorflow:loss = 5.86798e-06, step = 701 (0.145 sec)\n",
      "INFO:tensorflow:global_step/sec: 648.571\n",
      "INFO:tensorflow:loss = 1.94136e-06, step = 801 (0.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 637.106\n",
      "INFO:tensorflow:loss = 4.4497e-07, step = 901 (0.156 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /var/folders/66/y1hc77j572v71r0gm2r39rfr0000gn/T/tmpaqwcnezd/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.32125e-08.\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-22-05:55:08\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/66/y1hc77j572v71r0gm2r39rfr0000gn/T/tmpaqwcnezd/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-22-05:55:09\n",
      "INFO:tensorflow:Saving dict for global step 1000: average_loss = 2.12366e-08, global_step = 1000, loss = 8.49463e-08\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-22-05:55:10\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/66/y1hc77j572v71r0gm2r39rfr0000gn/T/tmpaqwcnezd/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-22-05:55:11\n",
      "INFO:tensorflow:Saving dict for global step 1000: average_loss = 0.00253751, global_step = 1000, loss = 0.01015\n",
      "train metrics: {'average_loss': 2.1236568e-08, 'loss': 8.4946272e-08, 'global_step': 1000}\n",
      "eval metrics: {'average_loss': 0.0025375069, 'loss': 0.010150027, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "# NumPy is often used to load, manipulate and preprocess data.\n",
    "import numpy as np\n",
    "\n",
    "# Declare list of features. We only have one numeric feature. There are many\n",
    "# other types of columns that are more complicated and useful.\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[1])]\n",
    "\n",
    "# An estimator is the front end to invoke training (fitting) and evaluation\n",
    "# (inference). There are many predefined types like linear regression,\n",
    "# linear classification, and many neural network classifiers and regressors.\n",
    "# The following code provides an estimator that does linear regression.\n",
    "estimator = tf.estimator.LinearRegressor(feature_columns=feature_columns)\n",
    "\n",
    "# TensorFlow provides many helper methods to read and set up data sets.\n",
    "# Here we use two data sets: one for training and one for evaluation\n",
    "# We have to tell the function how many batches\n",
    "# of data (num_epochs) we want and how big each batch should be.\n",
    "x_train = np.array([1., 2., 3., 4.])\n",
    "y_train = np.array([0., -1., -2., -3.])\n",
    "x_eval = np.array([2., 5., 8., 1.])\n",
    "y_eval = np.array([-1.01, -4.1, -7, 0.])\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=None, shuffle=True)\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_eval}, y_eval, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "\n",
    "# We can invoke 1000 training steps by invoking the  method and passing the\n",
    "# training data set.\n",
    "estimator.train(input_fn=input_fn, steps=1000)\n",
    "\n",
    "# Here we evaluate how well our model did.\n",
    "train_metrics = estimator.evaluate(input_fn=train_input_fn)\n",
    "eval_metrics = estimator.evaluate(input_fn=eval_input_fn)\n",
    "print(\"train metrics: %r\"% train_metrics)\n",
    "print(\"eval metrics: %r\"% eval_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
